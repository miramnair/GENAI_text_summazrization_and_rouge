{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ea1e9e9",
   "metadata": {},
   "source": [
    "<span style=\"font-size:20px\">**Dialogue Summarization Using Generative AI and ROUGE Score Analysis** </span>\n",
    "\n",
    "**Objective :** \n",
    "\n",
    "To summarize dialogues using various generative AI models and analyze the results using ROUGE scores.\n",
    "\n",
    "**Model Selection :**\n",
    "\n",
    "1) Amazon Titan Text Lite accessed through Amazon Bedrock\n",
    "2) LangChain gpt-3.5-turbo using Chat Messages \n",
    "3) LangChain gpt-3.5-turbo using Promp template\n",
    "4) HuggingFace's Flan-T5\n",
    "\n",
    "**Dataset :** https://huggingface.co/datasets/knkarthick/dialogsum\n",
    "\n",
    "**Evaluate Using ROUGE Scores** : Compute ROUGE scores (ROUGE-1, ROUGE-2, ROUGE-L) for each model's summaries against the reference summaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e06b15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets==2.17.0\n",
      "  Downloading datasets-2.17.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets==2.17.0) (3.13.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets==2.17.0) (1.22.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets==2.17.0) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets==2.17.0) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets==2.17.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets==2.17.0) (2.2.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets==2.17.0) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets==2.17.0) (4.66.2)\n",
      "Collecting xxhash (from datasets==2.17.0)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets==2.17.0) (0.70.16)\n",
      "Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.17.0)\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets==2.17.0) (3.9.3)\n",
      "Collecting huggingface-hub>=0.19.4 (from datasets==2.17.0)\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets==2.17.0) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets==2.17.0) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets==2.17.0) (4.10.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging->datasets==2.17.0) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->datasets==2.17.0) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->datasets==2.17.0) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->datasets==2.17.0) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.17.0) (1.16.0)\n",
      "Downloading datasets-2.17.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.6/536.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.6/402.6 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, fsspec, huggingface-hub, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.3.1\n",
      "    Uninstalling fsspec-2024.3.1:\n",
      "      Successfully uninstalled fsspec-2024.3.1\n",
      "Successfully installed datasets-2.17.0 fsspec-2023.10.0 huggingface-hub-0.23.4 xxhash-3.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pip in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (23.3.2)\n",
      "Collecting pip\n",
      "  Downloading pip-24.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading pip-24.1.1-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.3.2\n",
      "    Uninstalling pip-23.3.2:\n",
      "      Successfully uninstalled pip-23.3.2\n",
      "Successfully installed pip-24.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting transformers==4.27.2\n",
      "  Downloading transformers-4.27.2-py3-none-any.whl.metadata (106 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting evaluate==0.4.0\n",
      "  Downloading evaluate-0.4.0-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting rouge_score==0.1.2\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers==4.27.2) (3.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers==4.27.2) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers==4.27.2) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers==4.27.2) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers==4.27.2) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers==4.27.2) (2023.12.25)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers==4.27.2) (2.31.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.27.2)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers==4.27.2) (4.66.2)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from evaluate==0.4.0) (2.17.0)\n",
      "Requirement already satisfied: dill in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from evaluate==0.4.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from evaluate==0.4.0) (2.2.1)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from evaluate==0.4.0) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from evaluate==0.4.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate==0.4.0) (2023.10.0)\n",
      "Collecting responses<0.19 (from evaluate==0.4.0)\n",
      "  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting absl-py (from rouge_score==0.1.2)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rouge_score==0.1.2) (3.8.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rouge_score==0.1.2) (1.16.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate==0.4.0) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate==0.4.0) (0.6)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate==0.4.0) (3.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.27.2) (4.10.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.27.2) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers==4.27.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers==4.27.2) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers==4.27.2) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers==4.27.2) (2024.2.2)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk->rouge_score==0.1.2) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk->rouge_score==0.1.2) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->evaluate==0.4.0) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->evaluate==0.4.0) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->evaluate==0.4.0) (2024.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (4.0.3)\n",
      "Downloading transformers-4.27.2-py3-none-any.whl (6.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=ff16282cbb39ed6f4b3cbec754acd0067095947641012d731f598c183bac2fcb\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: tokenizers, absl-py, rouge_score, responses, transformers, evaluate\n",
      "Successfully installed absl-py-2.1.0 evaluate-0.4.0 responses-0.18.0 rouge_score-0.1.2 tokenizers-0.13.3 transformers-4.27.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#Install the dependencies\n",
    "\n",
    "%pip install -U datasets==2.17.0\n",
    "\n",
    "%pip install --upgrade pip\n",
    "%pip install --disable-pip-version-check \\\n",
    "    torch==1.13.1 \\\n",
    "    torchdata==0.5.1 --quiet\n",
    "\n",
    "%pip install \\\n",
    "    transformers==4.27.2 \\\n",
    "    evaluate==0.4.0 \\\n",
    "    rouge_score==0.1.2 \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b866bd27",
   "metadata": {},
   "source": [
    "### Model 1 : Amazon Titan Text Lite from Amazon Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f374e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import boto3, json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1304fb32",
   "metadata": {},
   "source": [
    "### Initializing AWS Bedrock Runtime Client\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf809814",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_runtime = boto3.client(service_name = \"bedrock-runtime\", region_name = \"us-east-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b43174",
   "metadata": {},
   "source": [
    "### Import the dataset from hugging face\n",
    "\n",
    "Upload some simple dialogues from the [DialogSum](https://huggingface.co/datasets/knkarthick/dialogsum) Hugging Face dataset. This dataset contains 10,000+ dialogues with the corresponding manually labeled summaries and topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5845531e",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "\n",
    "dataset = load_dataset(huggingface_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b07b4def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fa2989",
   "metadata": {},
   "source": [
    "### Print a couple of dialogues with their baseline summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6126f57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT DIALOGUE:\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT DIALOGUE:\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_indices = [40, 200]\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "\n",
    "for i, index in enumerate(example_indices):\n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print('INPUT DIALOGUE:')\n",
    "    print(dataset['test'][index]['dialogue'])\n",
    "    print(dash_line)\n",
    "    print('BASELINE HUMAN SUMMARY:')\n",
    "    print(dataset['test'][index]['summary'])\n",
    "    print(dash_line)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b86a40b",
   "metadata": {},
   "source": [
    "### Summarize the dialogue\n",
    "\n",
    "To simplify, let's randomly select index 40 and focus on summarizing that specific dialogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fe63294a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'modelId': 'amazon.titan-text-lite-v1', 'contentType': 'application/json', 'accept': 'application/json', 'body': '{\"inputText\": \"Summarize the conversation in a single line or two #Person1#: What time is it, Tom?\\\\n#Person2#: Just a minute. It\\'s ten to nine by my watch.\\\\n#Person1#: Is it? I had no idea it was so late. I must be off now.\\\\n#Person2#: What\\'s the hurry?\\\\n#Person1#: I must catch the nine-thirty train.\\\\n#Person2#: You\\'ve plenty of time yet. The railway station is very close. It won\\'t take more than twenty minutes to get there.\", \"textGenerationConfig\": {\"maxTokenCount\": 4096, \"stopSequences\": [], \"temperature\": 0, \"topP\": 1}}'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "input_text = dataset['test'][40]['dialogue']\n",
    "body = {\n",
    "    \"inputText\": f\"Summarize the conversation in a single line or two {input_text}\",\n",
    "    \"textGenerationConfig\": {\n",
    "        \"maxTokenCount\": 4096,\n",
    "        \"stopSequences\": [],\n",
    "        \"temperature\": 0,\n",
    "        \"topP\": 1\n",
    "    }\n",
    "}\n",
    "\n",
    "kwargs = {\n",
    "    \"modelId\": \"amazon.titan-text-lite-v1\",\n",
    "    \"contentType\": \"application/json\",\n",
    "    \"accept\": \"application/json\",\n",
    "    \"body\": json.dumps(body)\n",
    "}\n",
    "\n",
    "print(kwargs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562cef74",
   "metadata": {},
   "source": [
    "### Get the summarized text from the model and store in completion_bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bcb9ca78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '5db42d2e-1b58-41db-8339-449b30416705',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Wed, 03 Jul 2024 09:50:06 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '240',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': '5db42d2e-1b58-41db-8339-449b30416705',\n",
       "   'x-amzn-bedrock-invocation-latency': '1426',\n",
       "   'x-amzn-bedrock-output-token-count': '35',\n",
       "   'x-amzn-bedrock-input-token-count': '121'},\n",
       "  'RetryAttempts': 1},\n",
       " 'contentType': 'application/json',\n",
       " 'body': <botocore.response.StreamingBody at 0x7f85c3441a50>}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = bedrock_runtime.invoke_model(**kwargs)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "de6d5ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputTextTokenCount': 121,\n",
       " 'results': [{'tokenCount': 35,\n",
       "   'outputText': '\\nPerson 1 is rushing to catch a nine-thirty train, but Person 2 tells them they have plenty of time and that the railway station is close.',\n",
       "   'completionReason': 'FINISH'}]}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_body = json.loads(response.get('body').read())\n",
    "response_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "99b4c942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPerson 1 is rushing to catch a nine-thirty train, but Person 2 tells them they have plenty of time and that the railway station is close.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion_bedrock = response_body.get('results')[0].get('outputText')\n",
    "completion_bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8a9fd1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION :\n",
      "\n",
      "Person 1 is rushing to catch a nine-thirty train, but Person 2 tells them they have plenty of time and that the railway station is close.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_indices = [40]\n",
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "    \n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{dialogue}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION :\\n{completion_bedrock}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620989ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae55af41",
   "metadata": {},
   "source": [
    "### Model 2 : gpt-3.5-turbo from langchain and use Chat Messages to generate summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c8b8e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: langchain in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (0.2.6)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain) (2.0.29)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain) (0.2.11)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain) (0.2.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain) (0.1.83)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain) (1.22.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain) (2.8.0)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.10->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.10->langchain) (24.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.6)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain) (2.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: langchain_community in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (0.2.6)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain_community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain_community) (2.0.29)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain_community) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain_community) (0.2.6)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain_community) (0.2.11)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain_community) (0.1.83)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain_community) (1.22.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain_community) (2.31.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain_community) (8.2.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.3)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain<0.3.0,>=0.2.6->langchain_community) (0.2.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain<0.3.0,>=0.2.6->langchain_community) (2.8.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.10->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.10->langchain_community) (24.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (2024.2.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (4.10.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain_community) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.6->langchain_community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.6->langchain_community) (2.20.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting openai\n",
      "  Downloading openai-1.35.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from openai) (4.3.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from openai) (2.8.0)\n",
      "Requirement already satisfied: sniffio in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.20.0)\n",
      "Downloading openai-1.35.9-py3-none-any.whl (328 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.3/328.3 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: distro, openai\n",
      "Successfully installed distro-1.9.0 openai-1.35.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from tiktoken) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
      "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain\n",
    "%pip install langchain_community\n",
    "%pip install openai\n",
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33519900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "open_api_key=\"sk-proj-t####\"\n",
    "os.environ[\"OPENAI_API_KEY\"]=open_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "def7c687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import(\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b22b6118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#Person1#: What time is it, Tom?\\n#Person2#: Just a minute. It's ten to nine by my watch.\\n#Person1#: Is it? I had no idea it was so late. I must be off now.\\n#Person2#: What's the hurry?\\n#Person1#: I must catch the nine-thirty train.\\n#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech = dataset['test'][40]['dialogue']\n",
    "speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671b9fd4",
   "metadata": {},
   "source": [
    "### Get the summarized text from the model and store in completion_langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "eac19c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_messages=[\n",
    "    SystemMessage(content='You are an expert assistant with expertize in summarizing speeches'),\n",
    "    HumanMessage(content=f'Please provide a short and concise summary of the following speech in not more than one or two sentences:\\n TEXT: {speech}')\n",
    "]\n",
    "\n",
    "llm=ChatOpenAI(model_name='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3ec8ff21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens(speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "94d2d284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Person 1 realizes it is almost nine o'clock and needs to catch a train at 9:30, but Person 2 reassures them they have plenty of time as the train station is nearby and it will only take around twenty minutes to get there.\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion_langchain = llm.invoke(chat_messages).content\n",
    "completion_langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8432335a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION :\n",
      "Person 1 realizes it is almost nine o'clock and needs to catch a train at 9:30, but Person 2 reassures them they have plenty of time as the train station is nearby and it will only take around twenty minutes to get there.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_indices = [40]\n",
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "    \n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{dialogue}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION :\\n{completion_langchain}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911f6400",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c327fd9f",
   "metadata": {},
   "source": [
    "### Model 3 : gpt-3.5-turbo from langchain and use Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "18cb37ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "18c1f793",
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_template='''\n",
    "Write a summary of the following speech in not more than one or two sentences:\n",
    "Speech : `{speech}`\n",
    "\n",
    "'''\n",
    "prompt=PromptTemplate(\n",
    "    input_variables=['speech'],\n",
    "    template=generic_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c6b27490",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_prompt = prompt.format(speech=speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "08c9b469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens(complete_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bb3fe1",
   "metadata": {},
   "source": [
    "### Get the summarized text from the model and store in completion_langchain_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a8428acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Person 1 realizes it is later than they thought and needs to leave to catch the nine-thirty train, but Person 2 reassures them there is still plenty of time as the railway station is close by.'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "completion_langchain_prompt = chain.invoke({\"speech\": speech})\n",
    "completion_langchain_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "66e1373b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION :\n",
      "Person 1 realizes it is later than they thought and needs to leave to catch the nine-thirty train, but Person 2 reassures them there is still plenty of time as the railway station is close by.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_indices = [40]\n",
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "    \n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{dialogue}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION :\\n{completion_langchain_prompt}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0356ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "319b3a60",
   "metadata": {},
   "source": [
    "### Model 4 : Flan-T5 from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48ad2530",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "00518ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c95fe32",
   "metadata": {},
   "source": [
    "### Get the summarized text from the model and store in completion_hugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "15669a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation in not more than one or two lines.\n",
      "\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "\n",
      "Summary:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION:\n",
      "The train is about to leave.\n"
     ]
    }
   ],
   "source": [
    "index = 40\n",
    "\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation in not more than one or two lines.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "completion_hugging = tokenizer.decode(\n",
    "    original_model.generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        max_new_tokens=200,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION:\\n{completion_hugging}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92216ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c021c81",
   "metadata": {},
   "source": [
    "### Store the summarized text from each of the model into a dataframe. This would help us manually check the resluts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3dc51eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_baseline_summaries</th>\n",
       "      <th>bedrock_titan</th>\n",
       "      <th>langchain_gpt</th>\n",
       "      <th>langchain_prompt_gpt</th>\n",
       "      <th>hugging_flant5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#Person1# is in a hurry to catch a train. Tom ...</td>\n",
       "      <td>\\nPerson 1 is rushing to catch a nine-thirty t...</td>\n",
       "      <td>Person 1 realizes it is almost nine o'clock an...</td>\n",
       "      <td>Person 1 realizes it is later than they though...</td>\n",
       "      <td>The train is about to leave.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            human_baseline_summaries  \\\n",
       "0  #Person1# is in a hurry to catch a train. Tom ...   \n",
       "\n",
       "                                       bedrock_titan  \\\n",
       "0  \\nPerson 1 is rushing to catch a nine-thirty t...   \n",
       "\n",
       "                                       langchain_gpt  \\\n",
       "0  Person 1 realizes it is almost nine o'clock an...   \n",
       "\n",
       "                                langchain_prompt_gpt  \\\n",
       "0  Person 1 realizes it is later than they though...   \n",
       "\n",
       "                 hugging_flant5  \n",
       "0  The train is about to leave.  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "human_baseline_summaries = dataset['test'][40]['summary']\n",
    "summaries = [(human_baseline_summaries, completion_bedrock, completion_langchain, completion_langchain_prompt, completion_hugging)]\n",
    "df = pd.DataFrame(summaries, columns = ['human_baseline_summaries', 'bedrock_titan', 'langchain_gpt', 'langchain_prompt_gpt', 'hugging_flant5'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d0eb4c",
   "metadata": {},
   "source": [
    "### Calculate the ROUGE score for each of the model\n",
    "\n",
    "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics used for evaluating the quality of automatic summaries by comparing them to a set of reference summaries (typically human-generated). Here's what each ROUGE metric measures:\n",
    "\n",
    "1. **ROUGE-1 (unigram overlap):**\n",
    "   - Measures the overlap of unigrams (individual words) between the generated summary and the reference summaries.\n",
    "   - Example: If a reference summary contains \"good morning\" and the generated summary contains \"morning is good\", ROUGE-1 would count \"morning\" and \"good\" as overlapping unigrams.\n",
    "\n",
    "2. **ROUGE-2 (bigram overlap):**\n",
    "   - Measures the overlap of bigrams (sequences of two adjacent words) between the generated summary and the reference summaries.\n",
    "   - Example: If a reference summary contains \"good morning\" and the generated summary contains \"morning is good\", ROUGE-2 would count \"morning is\" and \"is good\" as overlapping bigrams.\n",
    "\n",
    "3. **ROUGE-L (longest common subsequence):**\n",
    "   - Measures the longest common subsequence (LCS) between the generated summary and the reference summaries. LCS is the longest sequence that can be found in both the generated and reference summaries without reordering the words.\n",
    "   - Example: If a reference summary is \"The cat sat on the mat\" and the generated summary is \"A cat sat on a mat\", ROUGE-L would find the longest common subsequence as \"cat sat on mat\".\n",
    "\n",
    "4. **ROUGE-Lsum (average ROUGE-L):**\n",
    "   - Measures the average of the ROUGE-L scores of the individual sentences in the generated summary against the reference summaries. This metric is useful for multi-sentence summaries.\n",
    "   - Example: If a generated summary consists of multiple sentences, ROUGE-Lsum calculates the average ROUGE-L score across all sentences.\n",
    "\n",
    "### Interpretation:\n",
    "\n",
    "- **Higher Scores**: A higher ROUGE score (close to 1.0) indicates a better overlap or similarity between the generated summary and the reference summaries, suggesting better quality in terms of content overlap.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f3be12f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEDROCK TITAN TEXT LITE MODEL:\n",
      "{'rouge1': 0.11627906976744186, 'rouge2': 0.0, 'rougeL': 0.11627906976744186, 'rougeLsum': 0.11627906976744186}\n",
      "LANGCHIAN OPENAI gpt-3.5-turbo MODEL:\n",
      "{'rouge1': 0.046511627906976744, 'rouge2': 0.0, 'rougeL': 0.046511627906976744, 'rougeLsum': 0.046511627906976744}\n",
      "LANGCHIAN OPENAI gpt-3.5-turbo MODEL WITH PROMPT TEMPLATE:\n",
      "{'rouge1': 0.08139534883720931, 'rouge2': 0.0, 'rougeL': 0.08139534883720931, 'rougeLsum': 0.08139534883720931}\n",
      "HUGGING FLANT5 MODEL:\n",
      "{'rouge1': 0.10714285714285714, 'rouge2': 0.0, 'rougeL': 0.10714285714285714, 'rougeLsum': 0.10714285714285714}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "min_length = min(len(completion_bedrock), len(human_baseline_summaries))\n",
    "completion_bedrock = completion_bedrock[:min_length]\n",
    "bedrock_model_results = rouge.compute(\n",
    "    predictions=completion_bedrock,\n",
    "    references=human_baseline_summaries[0:min_length],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "min_length = min(len(completion_langchain), len(human_baseline_summaries))\n",
    "completion_langchain = completion_langchain[:min_length]\n",
    "langchain_model_results = rouge.compute(\n",
    "    predictions=completion_langchain,\n",
    "    references=human_baseline_summaries[0:min_length],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "min_length = min(len(completion_langchain_prompt), len(human_baseline_summaries))\n",
    "completion_langchain_prompt = completion_langchain_prompt[:min_length]\n",
    "langchain2_model_results = rouge.compute(\n",
    "    predictions=completion_langchain_prompt,\n",
    "    references=human_baseline_summaries[0:min_length],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "min_length = min(len(completion_hugging), len(human_baseline_summaries))\n",
    "completion_hugging = completion_hugging[:min_length]\n",
    "hugging_model_results = rouge.compute(\n",
    "    predictions=completion_hugging,\n",
    "    references=human_baseline_summaries[0:min_length],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d78748",
   "metadata": {},
   "source": [
    "### Analysis:\n",
    "\n",
    "### ROUGE-1 Score Comparison: \n",
    "Among the models evaluated, the BEDROCK TITAN TEXT LITE MODEL achieved the highest ROUGE-1 score of 0.116. This indicates that it had the highest overlap in unigrams (individual words) with the reference summaries compared to the other models.\n",
    "\n",
    "### Other ROUGE Metrics: \n",
    "All models had a ROUGE-2 score of 0, indicating that none of them performed well in capturing bigram overlaps with the reference summaries. ROUGE-L and ROUGE-Lsum scores were comparable across the models, with slight variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cdd21360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEDROCK TITAN TEXT LITE MODEL:\n",
      "{'rouge1': 0.11627906976744186, 'rouge2': 0.0, 'rougeL': 0.11627906976744186, 'rougeLsum': 0.11627906976744186}\n",
      "\n",
      "\n",
      "LANGCHIAN OPENAI gpt-3.5-turbo MODEL:\n",
      "{'rouge1': 0.046511627906976744, 'rouge2': 0.0, 'rougeL': 0.046511627906976744, 'rougeLsum': 0.046511627906976744}\n",
      "\n",
      "\n",
      "LANGCHIAN OPENAI gpt-3.5-turbo MODEL WITH PROMPT TEMPLATE:\n",
      "{'rouge1': 0.08139534883720931, 'rouge2': 0.0, 'rougeL': 0.08139534883720931, 'rougeLsum': 0.08139534883720931}\n",
      "\n",
      "\n",
      "HUGGING FLANT5 MODEL:\n",
      "{'rouge1': 0.10714285714285714, 'rouge2': 0.0, 'rougeL': 0.10714285714285714, 'rougeLsum': 0.10714285714285714}\n"
     ]
    }
   ],
   "source": [
    "print('BEDROCK TITAN TEXT LITE MODEL:')\n",
    "print(bedrock_model_results)\n",
    "print('\\n')\n",
    "print('LANGCHIAN OPENAI gpt-3.5-turbo MODEL:')\n",
    "print(langchain_model_results)\n",
    "print('\\n')\n",
    "print('LANGCHIAN OPENAI gpt-3.5-turbo MODEL WITH PROMPT TEMPLATE:')\n",
    "print(langchain2_model_results)\n",
    "print('\\n')\n",
    "print('HUGGING FLANT5 MODEL:')\n",
    "print(hugging_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ba7755",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
